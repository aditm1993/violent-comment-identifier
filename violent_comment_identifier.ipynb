{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "violent comment identifier",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMGp2qH1bOku",
        "outputId": "63d63710-f710-4331-f410-737a8b7b4cfc"
      },
      "source": [
        "import keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, LSTM, GlobalMaxPool1D, Bidirectional\n",
        "from keras.layers import BatchNormalization, concatenate\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6R17gG7pwjf",
        "outputId": "dfbdd2f3-8c56-444a-8b19-b4f06860ccff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9eTglaUpwm1"
      },
      "source": [
        "#train = pd.read_csv('../comment/train_wf.csv')\n",
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/comment/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/comment/test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db5SNLcKbPRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6b1e0e-7b46-40af-ab31-915ed740cdda"
      },
      "source": [
        "#nb_features = 20000\n",
        "max_length = 100\n",
        "\n",
        "Y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
        "print (Y_train.shape)\n",
        "\n",
        "simple_tokens = train.comment_text.apply(gensim.utils.simple_preprocess)\n",
        "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
        "tokenizer = gensim.models.phrases.Phraser(phrases)\n",
        "tokenized_text = list(tokenizer[simple_tokens])\n",
        "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)\n",
        "\n",
        "word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159571, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVbQvv2bPVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfbefa7-4ddf-4c91-e1b1-835ea862c1fa"
      },
      "source": [
        "print(train.comment_text[1])\n",
        "print(tokenized_text[1])\n",
        "word2vec.wv.most_similar('shit')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
            "['aww', 'he', 'matches', 'this', 'background', 'colour', 'seemingly', 'stuck', 'with', 'thanks', 'talk', 'january_utc']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('crap', 0.8439592123031616),\n",
              " ('fucking', 0.816267192363739),\n",
              " ('bullshit', 0.7952636480331421),\n",
              " ('bitch', 0.7541518211364746),\n",
              " ('asshole', 0.7289239168167114),\n",
              " ('garbage', 0.720348060131073),\n",
              " ('dick', 0.7169774174690247),\n",
              " ('loser', 0.7101936340332031),\n",
              " ('scum', 0.7099797129631042),\n",
              " ('fuck', 0.7084515690803528)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrOx2mk_bPXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9493b92e-66dc-4488-8c81-7bd238f9d321"
      },
      "source": [
        "# features = np.zeros((len(tokenized_text), word2vec.vector_size))\n",
        "# for i, tokens in enumerate(tokenized_text):\n",
        "#     tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
        "#     if tokens:\n",
        "#         features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)\n",
        "\n",
        "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]\n",
        "MAX_SEQ_LEN = 100\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)\n",
        "\n",
        "max_idx = max(c for d in docs for c in d)\n",
        "print (X_train.shape, max_idx)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159571, 100) 185872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OJxwCzo4tyr",
        "outputId": "8555c1bd-abb7-4e08-b4ee-59743fc969c9"
      },
      "source": [
        "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ \n",
        "                      [word2vec.wv[corpus_dict[idx]]\n",
        "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
        "                      else np.random.normal(size=word2vec.vector_size)\n",
        "                      for idx in range(max_idx)])\n",
        "embeddings.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(185873, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOzJ_G5c4t2B",
        "outputId": "5d3349e1-7d1f-4f86-e09c-f5b630bc954f"
      },
      "source": [
        "inp_text = Input(shape=(max_length, ))\n",
        "\n",
        "x= Embedding(max_idx + 1, word2vec.vector_size, weights=[embeddings], input_length=MAX_SEQ_LEN)(inp_text)\n",
        "x = Bidirectional(LSTM(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "\n",
        "x = Dense(75, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=[inp_text], outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 100, 100)          18587300  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 400)          481600    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 75)                30075     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 456       \n",
            "=================================================================\n",
            "Total params: 19,099,431\n",
            "Trainable params: 19,099,431\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vrbr9loEye",
        "outputId": "803a9096-0719-4992-c0a1-7bfc2f6cffc1"
      },
      "source": [
        "file_path=\"/content/drive/My Drive/Colab Notebooks/comment/bilstm_w2v_weight.h5\"\n",
        "# model.load_weights(file_path)\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
        "callbacks_list = [checkpoint, early] \n",
        "\n",
        "hist = model.fit([X_train], Y_train, epochs=3, batch_size=32, validation_split=0.1, shuffle=False, callbacks = callbacks_list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4368/4488 [============================>.] - ETA: 1:39 - loss: 0.0696 - accuracy: 0.8847"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYeYYR1QoE2z"
      },
      "source": [
        "print(hist.history.keys())\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSPIjkGpoE52"
      },
      "source": [
        "def comment_to_sequential_input(comment):\n",
        "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n",
        "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n",
        "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]\n",
        "\n",
        "X_test = np.array([comment_to_sequential_input(doc) for doc in test.comment_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slufLRdAoE8k"
      },
      "source": [
        "model.load_weights(file_path)\n",
        "Y_test = model.predict([X_test], batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tagZecucoE_o"
      },
      "source": [
        "final_test = pd.read_csv(\"../data/sample_submission.csv\")\n",
        "final_test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = Y_test\n",
        "final_test.to_csv(\"bilstm_w2v_weight.csv\", index=False)\n",
        "final_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq7jrhMatt12"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kes97_M4t5H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQYfIsIt07r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPe15E7m4t78"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeQiRgV5bPaa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3B9PZl_bPdN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-PLeWRbPgR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWD9cK90bPjB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdS8jBBTbPl2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1cAQJWtbPof"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZUFBmv5bPrK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9URuAY_bPuD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLJFmaagbPwr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gitKR5T2bP2a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmWW0WPYbP67"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTUtMiB-bP9u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3kH5rcibQAq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GNufEvLbQDT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}